# FTF-CRTT
This repository provides code and instructions for running the Face-to-Face Competetive Reaction Time Task (FTF-CRTT) first described in our academic article[add hyperlink]. Please cite this paper in any publications including our code or design. Unlike the classical CRTT, ours is designed to be run with two participants. There are simpler versions of the CRTT available elsewhere designed to be run with just a single person (and a fake "opponent"), which are likey a better option if you wish to run an individual CRTT study. For example, available [here](https://www.millisecond.com/download/library/competitivereactiontime). Our code allows the winner of each round to send a "sound blast" to the loser.

## Executable
We have provided an executable version of our experiment using [PyInstaller](https://github.com/pyinstaller). You can use this to view our experiment without interfacing with Python. This executable will also allow you to replicate our study. If you want to customize our code to extend our research, you will need to modify our script in python. Instructions to get started are below.

## Installation
The code is only compatible with Windows. I wrote it in Python3 on Windows 11. I do not know if it is compatible with other versions of Python or Windows.

### Pydub and SimpleAudio
The code requires you to install [Pydub](https://github.com/jiaaro/pydub) and [SimpleAudio](https://github.com/hamiltron/py-simple-audio). They can be difficult to get working, but with enough trial and error, it will work. You may have to import SimpleAudio into your working directory manually.

### Our audio file
The audio file we used for blasts is available in this repository (radio_static.mp3). If you plan to run our code through python, you will need to update the audio_file_path to point to the audio file on your computer.

## Configuration in the lab
Our lab is split into two rooms: the control room (where our computer running the program is) and the experimentation room (where the participants play the CRTT), as illustrated [here](https://github.com/AnnahGrace01/FTF-CRTT/blob/main/Arrangment%20of%20participants.pdf). Put simply, our code runs off of one computer and is displayed to both participants on two monitors simultaneously. Both participants see the same thing, and we leverage audio channels to send sound to only one participant at a time (expanded on bellow).

### Hardware
Our hardware consists of one computer running the code, an AUX. splitter, an HDMI splitter, a USB hub, two monitors, two keyboards, two pairs of headphones, and two big red buttons, as illustrated [here](https://github.com/AnnahGrace01/FTF-CRTT/blob/main/Hardware%20Diagram.pdf). The big red buttons emulate the "1" or "2" key ("1" for Player 1, etc.). Player 1's headphones are attached to the right audio channel, and Player 2 gets the left audio channel. **NOTE.** Our code needs to play audio on only one channel at a time. Your Windows computer may try to "make up for" the single-channel audio by mirroring it to the other channel. If this happens (audio comes out of both headphones), you will need to find the setting that controls this and disable it. It may be called "audio enhancements".

### Calibrating sound
It is important that you know the volume levels coming out of your experiment at each blast level. The precise volume output will entirely depend on your experimental set up, cable length, etc. This means that you need to calibrate the audio to you exact set up. You can do this by editing the volume modifiers in lines 92 to 99 of the main script. You may have difficulty reaching an output of 110 dBs. This is why we use in-line amplifiers. Note that every time you run the experiment, you will need to have your computer audio set to the same volume (probably 100%).

### Our camera
We recorded participants during the game. To do this, we used an [Insta360 ONE X2](https://www.insta360.com/product/insta360-onex2) placed in between participants. We synced the video to our experiment offline in the original study. We have since improved to code to start and stop recording over wifi, and sync the time more precisely. We will include this code once we are ready to publish it in this repository, along with a version without camera integration for those not using the Insta360 ONE X2. If you are looking for a good 360 camera for dyadic research, we have had success with Insta360. You should note, however, that their support for Python integration is almost non-existent.

## Extention, Customization, Standardization, and Analysis
We would love for you to modify our code and produce cool research. If you publish research with a modified version of our code and you are willing to share your modifications, please create a fork for the project and then make a pull request.

### Standardization of the CRTT
Before you design and employ a modification to the CRTT, you should know that there is a lot of debate in the field about standardization. We designed our FTF-CRTT with [Elson et al.'s (2014)](https://doi.org/10.1037/a0035569) recommendations in mind (available for free [here](https://www.researchgate.net/publication/259845770_Press_CRTT_to_Measure_Aggressive_Behavior_The_Unstandardized_Use_of_the_Competitive_Reaction_Time_Task_in_Aggression_Research)). Specifically, we use volume as a measure of aggression only, we did not include duration. As recommended, our blast level input scale is a range from 1-8, we did not include a 0 aggression response. Blast level 1 is 75dB, and each level is 5dB higher than the last (such that 8=110 dB). If you wish, you can change all of this by customizing our provided code.

### Analysis
There is a lot of debate in the field about the usefulness of the CRTT as a measure of aggression. Some people use only the first blast level selection and call it "unprovoked aggression", and then ignore the rest of the data they collected. Others take a mean of blast selection on every round and consider it a measure of reactive aggression, etc. Because we are interested in reactive aggression, and reactive aggression requires negative emotional arousal, we recorded and affect coded emotion during the game and included only rounds where people were highly negative (details in our article). This is why our code includes time readouts for Time To Button Press (TTBP) and Time To Blast Initiate (TTBI). If you choose not to record affect, you can probably ignore these outputs. If you are interested in other temporal relationships, you can add and modify the time locking. Finally, we are working on producing an end-to-end program to allow researchers to automatically analyze affect data using the Facial Action Coding System ([FACS](https://local.psy.miami.edu/faculty/dmessinger/c_c/rsrcs/rdgs/emot/FACSChapter_SAGEEncyclopedia.pdf)) if they are not certified to use the FACS. This program will provide a frame-by-frame continuous output of basic emotions on a 0-5 scale so that you do not interface with Action Units directly. With that being said, we still recommend that anyone who wants to meaningfully use facial action data to conduct affect research become certified and proficient in the FACS so that you *could* hand-code data intelligently even if you never do. While the FACS is a feat of behavioural science, it also has many pitfalls and quirks that effect analysis and interpretation, and you can only fully understand these limitations and oddities if you know the system inside and out.
